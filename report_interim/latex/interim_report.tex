% !TEX root = interim_report.tex
\documentclass[11pt]{report}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

% --- START OF CLASS FILE CONTENT ---
\makeatletter
\usepackage[paperwidth=21cm,paperheight=29.7cm,includehead,headheight=1.5cm,pdftex,hmargin={3cm,2.5cm},vmargin={0cm,2cm},]{geometry} 
\usepackage{fancyhdr}
\setlength{\headheight}{1.5cm}
\setlength{\parindent}{0cm} 
\renewcommand{\baselinestretch}{1.2}
\parskip=1em


\renewcommand{\contentsname}{Table of Contents}

\def\ps@plain{%
      \let\@oddhead\@empty
      \def\@oddfoot{\normalfont\hfil\small\textsf{Page \thepage~of~\pageref{endpage}}\hfil}%
      \def\@evenfoot{\normalfont\hfil\small\textsf{Page \thepage~of~\pageref{endpage}}\hfil}}


\renewcommand{\normalsize}{\fontsize{11pt}{11pt}\selectfont}
\renewcommand{\title}[1]{{\sffamily\Huge #1\par}}
\renewcommand{\author}[1]{{\sffamily\Huge #1\par}}
\newcommand{\subtitle}[1]{\textsf{\textbf{\Large{#1}}}}
\newcommand{\abstractheading}[1]{\textsf{\textbf{\LARGE{#1}}}}
\newcommand{\code}[1]{\texttt{\footnotesize{#1}}}

\renewcommand{\section}{\@startsection
{section}%                    % the name
{1}%                          % the level
{0mm}%                        % the indent
{6mm}%             % the beforeskip
{4.2mm}%           % the afterskip
{\LARGE\bfseries\sffamily}}  % the style

\renewcommand{\subsection}{\@startsection
{subsection}%                    % the name
{2}%                          % the level
{0mm}%                        % the indent
{4mm}%             % the beforeskip
{1.1mm}%           % the afterskip
{\Large\bfseries\sffamily}}  % the style

\renewcommand{\subsubsection}{\@startsection
{subsubsection}%                    % the name
{3}%                          % the level
{0mm}%                        % the indent
{4.2mm}%             % the beforeskip
{1.1mm}%           % the afterskip
{\normalsize\bfseries\sffamily}}  % the style

\renewcommand\chapter{\if@openright\cleardoublepage\else\clearpage\fi
                    \thispagestyle{plain}%
                    \global\@topnum\z@
                    \@afterindentfalse
                    \secdef\@chapter\@schapter}

%% Chapter headings should be at the top of the page.
\def\@makechapterhead#1{%
  { \parindent \z@ \raggedright \normalfont
    %\centering
    \ifnum \c@secnumdepth >\m@ne
        \huge\textsf{\@chapapp\space \thechapter:}
        % \par\nobreak
        %\vskip 20\p@
    \fi
    \interlinepenalty\@M
    \huge \bfseries \textsf{#1}\par\nobreak
%    \rule{5cm}{0.5pt}
    \vskip 20\p@
  } }
  
\def\@makeschapterhead#1{%
  %\vspace*{50\p@}%
  { \parindent \z@ \raggedright
    %\centering
    \normalfont
    \interlinepenalty\@M
    \huge \bfseries  \textsf{#1}\par\nobreak
%    \rule{5cm}{0.5pt}
    \vskip 20\p@

  }}
  
 \renewenvironment{abstract}{%
      \chapter*{\abstractname}%
      \addcontentsline{toc}{chapter}{\abstractname}
 }
     
     
\makeatletter
\renewcommand{\l@chapter}{\bfseries\@dottedtocline{1}{0em}{2.3em}}
\renewcommand{\l@section}{\normalfont\@dottedtocline{2}{2em}{2.3em}}
\renewcommand{\l@subsection}{\normalfont\@dottedtocline{3}{2em}{2.3em}}
\renewcommand{\l@subsubsection}{\normalfont\@dottedtocline{4}{2em}{2.3em}}
\makeatother

\def\maketitle{%
  \begin{titlepage}%
  \centering
  \title{Final Year Project Report}\vspace{0.25cm}
  \Large \textbf{\fullOrHalfUnit~-~\finalOrInterim}\\[0.5cm]\rule{4cm}{1pt}\\[0.7cm]
  \title{\textbf{\projecttitle}}\vspace{1cm}
  \author{\LARGE \studentname}\vspace{0.5cm}\rule{4cm}{1pt}\\[0.5cm]
  \textsf{\Large  A report submitted in part fulfilment of the degree of\\[0.5cm]
  \textbf{\degree}}\\[0.5cm]
  \textsf{\Large  \textbf{Supervisor:}  \supervisorname}\\[2cm]
  \includegraphics[height=5cm]{logo}\\[2cm]
  \textsf{\Large Department of Computer Science\\
  Royal Holloway, University of London\\\vfill
  \normalsize \today}
  \end{titlepage}%
}

\pagestyle{fancyplain}
\lhead{\footnotesize \shortprojecttitle}
\rhead{\footnotesize \studentname}
\renewcommand{\headrulewidth}{0pt}

\usepackage{tocloft}
\renewcommand{\cftchapfont}{\sffamily}
\renewcommand{\cftsecfont}{\sffamily}
\renewcommand{\cftchappagefont}{\sffamily}
\renewcommand{\cftsecpagefont}{\sffamily}
\renewcommand{\cftchapleader}{\sffamily\cftdotfill{\cftsecdotsep}}
\setlength{\cftbeforesecskip}{\cftbeforechapskip}
\setcounter{tocdepth}{1}
\makeatother
% --- END OF CLASS FILE CONTENT ---


%%%%%%%%%%%%%%%%%%%%%%
%%% Input project details
\def\studentname{Devansh Dev}
\def\reportyear{2025/2026}
\def\projecttitle{Comparative Evaluation of Deep Learning Architectures for Audio Recognition Tasks}
\def\shortprojecttitle{Comparative Evaluation of DL Architectures}
\def\supervisorname{Dr. Zhang}
\def\degree{BSc (Hons) in Computer Science}
\def\fullOrHalfUnit{Full Unit}
\def\finalOrInterim{Interim Report}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%
%%% Declaration

\chapter*{Declaration}

This report has been prepared on the basis of my own work. Where other published and unpublished source materials have been used, these have been acknowledged.

\vskip3em

Word Count: Approximately 6,200 words

\vskip3em

Student Name: \studentname

\vskip3em

Date of Submission: December 2025

\vskip3em

Signature:



%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
\tableofcontents\pdfbookmark[0]{Table of Contents}{toc}

%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract

\begin{abstract}

Environmental sound recognition is a fundamental component of modern artificial intelligence systems. This interim report presents a comparative evaluation of deep learning architectures for audio classification, focusing on the effectiveness of attention mechanisms in Convolutional Neural Networks (CNNs). We investigate whether integrating Squeeze-and-Excitation (SE) and Convolutional Block Attention Modules (CBAM) into a ResNet-50 backbone enhances or decreases performance compared to standard baselines (AlexNet, ResNet-50) on environmental sound (ESC-50) and speech emotion (EmoDB) datasets. The methodology employs transfer learning from ImageNet-pretrained weights, with audio signals converted to 224×224 Mel-spectrograms through a consistent preprocessing and augmentation pipeline. Initial results indicate that transfer learning provides a significant performance boost over simpler architectures (ResNet-50 achieving 89.9\% on ESC-50 vs. 74.5\% for AlexNet). Surprisingly, the addition of attention mechanisms showed limited benefits; while SE-Net achieves 88.8\% $\pm$2.4\%, CBAM exhibited severe training instability with mean accuracies of 73.4\% $\pm$10.9\% on ESC-50 and 80.2\% $\pm$19.3\% on EmoDB. This suggests that standard ResNet-50 features may already capture the necessary discriminative information and that spatial attention (CBAM) may introduce inappropriate inductive biases for spectrogram data. The models also demonstrate strong generalization on the EmoDB dataset with baseline accuracy exceeding 95\%. The report describes the experimental framework, discusses technical challenges such as data leakage prevention and attention mechanism integration, and outlines future work for the final dissertation. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%
%%% Project Specification

\chapter*{Project Specification}
\addcontentsline{toc}{chapter}{Project Specification}

\textbf{Project Title:} Comparative Evaluation of Deep Learning Architectures for Audio Recognition Tasks

\textbf{Student:} Devansh Dev

\textbf{Supervisor:} Dr. Zhang

\textbf{Project Type:} Full Unit Final Year Project

\section*{Aims}

The aim of this project is to conduct a systematic comparative evaluation of deep learning architectures for audio classification tasks, with particular focus on the possible effectiveness of
attention mechanisms when applied to spectrogram-based representations.
\section*{Objectives}

\begin{enumerate}
    \item Implementation and evaluation of baseline CNN architectures (custom CNN, AlexNet, ResNet-50) for audio classification using transfer learning from ImageNet.
    \item To integrate attention mechanisms (SE and CBAM) into ResNet-50 and evaluate their impact on classification performance.
    \item Conduct experiments on multiple audio datasets as to cross-validate across domains: ESC-50 (environmental sounds) and EmoDB (speech emotion recognition).
    \item Perform rigorous statistical analysis with multiple random seeds to ensure reproducibility and quantify performance variance.
    \item Analyze per-class performance and identify systematic failure patterns through confusion matrix analysis.
    \item Document technical challenges, solutions, and then reflect moving forward with audio tasks.
    \item (Future) Extend evaluation to visual recognition tasks and conduct cross-modal comparison.
\end{enumerate}
\newpage
\section*{Deliverables}

\textbf{Interim Report (Current):}
\begin{itemize}
    \item Literature review and background theory
    \item Methodology and experimental design
    \item Implementation of baseline and attention-augmented models
    \item Experimental results on ESC-50 and EmoDB datasets
    \item Project timeline and diary
\end{itemize}

\textbf{Final Dissertation:}
\begin{itemize}
    \item Extended experiments with additional datasets and architectures
    \item Attention visualization using Grad-CAM
    \item Ablation studies on attention mechanism components
    \item Comparison with state-of-the-art published results
    \item Cross-modal evaluation (audio vs. visual recognition)
    \item Comprehensive analysis and recommendations
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%
%%% Introduction
\chapter{Introduction}

Audio classification is one of the most fundamental tasks in machine learning domains, which facilitates advancements in the fields of speech recognition, music information retrieval, environmental monitoring, and assistive technologies for the hearing impaired.

Unlike classical signal processing techniques based on manually-crafted features, deep learning techniques are able to automatically learn hierarchical representations directly from audio data. However, raw audio waveforms pose certain challenges: raw audio is high-dimensional data, temporally extended, and contains information at multiple time scales.


A common approach is to transform audio into sound images like spectrograms, which transcode both time and frequency information in a two-dimensional format. This allows for the implementation of computer vision methods, specifically Convolution Neural Networks (CNN), which have been incredibly successful in image classification problems. Transfer learning from large scale image datasets such as ImageNet has been shown to be effective even with audio spectrograms, in spite of the difference between the domains in which natural images and acoustic patterns exist.

\section{Motivation}

Despite the fact that the canonical CNN architecture of AlexNet and ResNet have achieved high performance on well-established audio-classification benchmarks, the recent advances in attention mechanisms have the potential to help improve it even more. These processes allow networks to selectively pay attention to information features and ignore irrelevant information features. Squeeze-and-Excitation (SE) blocks perform channel-wise attention, recalibrating feature maps to emphasize discriminative channels. Convolutional Block Attention Module (CBAM) extends that by introducing spatial attention that allows the models to focus on particular time-frequency areas in spectrograms. 

However, it remains unclear whether these attention mechanisms, originally designed for natural images, provide consistent benefits for audio spectrograms. Spectrograms have structured axes (time and frequency) with physical meaning, unlike the more arbitrary spatial structure of natural images. This raises the question: \textit{Do attention mechanisms that excel at natural image classification transfer effectively to audio classification tasks?}

\section{Research Questions}

This project investigates and aims to answer the following research questions:

\begin{enumerate}
    \item To what extent can transfer learning from ImageNet-pretrained models be effective on audio classification tasks when using spectrogram representations? 
    \item Are attention mechanisms, that is, SE and CBAM, better at classification over baseline resnets like ResNet-50? 
    \item How do these architectures generalize across different audio domains (environmental sounds vs. speech emotion)?
    \item What are the failure modes and weaknesses of attention mechanisms on spectrogram-based classification? 
\end{enumerate}

\section{Objectives}

The primary objectives of this interim report are:

\begin{itemize}
    \item Have strong experimental baselines that use traditional CNN architectures (Baseline CNN, AlexNet, ResNet50). 
    \item Implement and assess attention mechanisms (SE, CBAM) in the ResNet -50 architecture. 
    \item Conduct rigorous experiments on two heterogeneous audio samples: ESC50 (sounds in the environment) and EmoDB (speech emotion).
    \item Analyze per-class performance and identify systematic failure patterns.
    \item Document technical challenges and solutions for reproducible research.
\end{itemize}

\section{Contributions}

This interim report presents the following contributions:

\begin{itemize}
    \item A comprehensive comparative evaluation of CNN architectures and attention mechanisms for audio classification.
    \item Empirical evidence that standard ResNet-50 features may already capture sufficient discriminative information for spectrograms, with attention mechanisms providing limited additional benefit.
    \item Identification of training instability issues with CBAM on spectrogram data, suggesting that spatial attention may introduce inappropriate inductive biases.
    \item A reproducible experimental framework with proper data leakage prevention and multi-seed validation.
    \item Cross-dataset validation demonstrating generalization from environmental sounds to speech emotion recognition.
\end{itemize}

\section{Report Structure}

The remainder of this report is organized as follows:

\textbf{Chapter 2: Background Theory} provides the theoretical foundations, covering audio representation, CNN architectures, transfer learning, and attention mechanisms.

\textbf{Chapter 3: Methodology} describes the experimental setup, including datasets, model architectures, training procedures, and evaluation metrics.

\textbf{Chapter 4: Implementation} details the software architecture, data pipeline, and technical challenges encountered during development.

\textbf{Chapter 5: Results and Analysis} presents quantitative results and per-class performance analysis across both datasets.

\textbf{Chapter 6: Planning and Schedule} outlines the original project plan versus the actual execution timeline.

\textbf{Chapter 7: Project Diary} documents the chronological progression of work and how the project evolved from September 2025 to December 2025.

%%%%%%%%%%%%%%%%%%%%%%
%%% Background Theory
\chapter{Background Theory}
\label{ch:background}

This chapter provides the theoretical foundations for understanding deep learning architectures applied to audio classification tasks.

\section{Audio Representation and Spectrograms}

\subsection{Short-Time Fourier Transform}

Audio signals are one-dimensional time-series data. In order to simultaneously have the time and frequency information we transform audio to spectrograms with the Short-Time Fourier Transform (STFT):
\begin{equation}
X(m, \omega) = \sum_{n=-\infty}^{\infty} x[n] w[n - m] e^{-j\omega n}
\end{equation}

where $x[n]$ is the discrete audio signal, $w[n]$ is a window function (typically Hann window), $m$ is the time frame index, and $\omega$ is the angular frequency. The STFT provides a time-frequency representation by using Fourier transform of short and overlapping signal segments. 

\subsection{Mel-Frequency Spectrograms}

Human hearing is sensitive to frequency, and thus perceives frequencies on a logarithmic scale, and not a linear one. The mel scale~\cite{stevens1937scale} provides a perceptually-motivated frequency representation:

\begin{equation}
m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)
\end{equation}

where f and m are frequency in Hz and mels respectively. Triangular mel-scale filterbanks on the STFT magnitude spectrum give us mel-spectrograms, which we scale to the logarithmic scale (decibels) to provide a better dynamic range:
\begin{equation}
S_{dB} = 10 \log_{10}(S_{mel} + \epsilon)
\end{equation}

where $\epsilon$ is a small constant (typically $10^{-10}$) to avoid numerical instability from $\log(0)$.

The result of this transformation is a 2-D image in which the horizontal axis is time, the vertical axis is mel-frequency bins, and the value of the pixel is the energy. Using that visual format, it is possible to apply computer-vision techniques to audio classification.

\section{Convolutional Neural Networks}

\subsection{Convolution Operation}

Convolutional Neural Networks (CNNs)~\cite{lecun1998gradient} apply learnable filters to extract hierarchical features from input data. For a 2D input $I$ and filter kernel $K$, the discrete convolution operation is defined as:

\begin{equation}
(I * K)(i, j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m, n)
\end{equation}

In the case of spectrograms, local time-frequency patterns are pulled out by convolution. Simple shapes such as edges and textures are often picked by the first convolutional layers, and deeper conv layers begin to pick up more interesting objects such as harmonic patterns and time series. 

 

\subsection{Pooling Operations}

Pooling layers reduce spatial dimensions while retaining important features. Max pooling selects the maximum value in each local region:

\begin{equation}
y_{i,j} = \max_{(m,n) \in R_{i,j}} x_{m,n}
\end{equation}

where $R_{i,j}$ is the pooling region. Average pooling computes the mean instead. Pooling provides translation invariance and reduces computational cost.

\subsection{Batch Normalization}

Batch Normalization~\cite{ioffe2015batch} normalizes layer inputs to stabilize training and enable higher learning rates:

\begin{equation}
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
\end{equation}

\begin{equation}
y = \gamma \hat{x} + \beta
\end{equation}

where $\mu_B$ and $\sigma_B^2$ are the batch mean and variance, and $\gamma$ and $\beta$ are learnable scale and shift parameters.

\subsection{Activation Functions}

Non-linear activation functions enable neural networks to learn complex patterns. The Rectified Linear Unit (ReLU) is widely used:

\begin{equation}
\text{ReLU}(x) = \max(0, x)
\end{equation}

ReLU addresses the vanishing gradient problem and provides computational efficiency compared to sigmoid or tanh activations.

\section{Deep Learning Architectures}
Deep learning architectures represent a category of neural network frameworks that are typically designed to execute complex neural network tasks at large scale.

\subsection{AlexNet}
The effectiveness of deep convolutional neural networks (CNNs) in image classification was demonstrated by AlexNet~\cite{krizhevsky2012imagenet}, which won the ImageNet 2012 competition. The architecture consists of five convolutional layers followed by three fully-connected layers, with regularisation applied through dropout and Rectified Linear Unit (ReLU) activations.

Key innovations introduced by AlexNet include:
\begin{itemize}
    \item Use of ReLU activation instead of the hyperbolic tangent function, which accelerated training
    \item Overlapping max-pooling
    \item Dropout regularisation with probability 0.5
    \item Data augmentation techniques
\end{itemize}

\subsection{ResNet-50}
Residual Networks (ResNet)~\cite{he2016deep} addressed the degradation problem in very deep networks by introducing skip connections. The core residual block is defined as:
\begin{equation}
    y = F(x, \{W_i\}) + x
    \label{eq:resnet_residual}
\end{equation}
where $F(x, \{W_i\})$ is the residual mapping to be learned and $x$ is added via an identity skip connection. This allows effective gradient propagation, enabling training of networks with 50, 101, or 152 layers.

ResNet-50 consists of 50 layers organised into four stages, each containing multiple residual blocks. It employs bottleneck blocks (1×1, 3×3, 1×1 convolutions) to reduce computational cost while preserving representational power.

\section{Transfer Learning}
\subsection{Pre-training and Fine-tuning}
Transfer learning leverages knowledge from large-scale datasets such as ImageNet~\cite{deng2009imagenet} (1.2 million images across 1,000 categories) to improve performance on target tasks with limited data. Despite the domain difference between natural images and audio spectrograms, low-level (edges, textures) and mid-level (patterns, shapes) features transfer effectively.

The process typically involves two stages:
\begin{itemize}
    \item \textbf{Phase 1 — Feature Extraction (Linear Probing):} Freeze all convolutional layers and train only the final classification layer.
    \item \textbf{Phase 2 — Fine-tuning:} Unfreeze selected or all layers and train with a lower learning rate for task-specific optimisation.
\end{itemize}

\subsection{Domain Adaptation}
Applying ImageNet-pretrained models to audio spectrograms is cross-domain transfer learning. Spectrograms differ from natural images in:
\begin{itemize}
    \item Structured time and frequency axes with physical meaning
    \item More consistent and repetitive trends
    \item Lack of colour information (grayscale)
\end{itemize}
Nevertheless, experimental results show ImageNet pre-training provides a strong baseline for spectrogram-based audio classification~\cite{hershey2017cnn}.

\section{Attention Mechanisms}
Attention mechanisms allow neural networks to focus on informative features while suppressing irrelevant ones. This section covers two key mechanisms for CNNs.

\subsection{Squeeze-and-Excitation (SE) Blocks}
SE blocks~\cite{hu2018squeeze} perform channel-wise attention by modelling inter-channel dependencies.

\subsubsection{Architecture}
For an input feature map of dimensions $H \times W \times C$:
\begin{enumerate}
    \item \textbf{Squeeze:} Global average pooling aggregates spatial information:
    \begin{equation}
        z_c = \frac{1}{H W} \sum_{i=1}^{H} \sum_{j=1}^{W} x_c(i, j)
    \end{equation}
    producing a channel descriptor $z \in \mathbb{R}^C$.
    \item \textbf{Excitation:} A bottleneck fully-connected network learns channel weights:
    \begin{equation}
        s = \sigma(W_2 \delta(W_1 z))
    \end{equation}
    where $W_1$ reduces to $C/r$ (r=16 typically), $W_2$ restores to C, $\delta$ is ReLU, $\sigma$ is sigmoid.
    \item \textbf{Scale:} Recalibrate features channel-wise: $\tilde{x}_c = s_c \cdot x_c$.
\end{enumerate}

\subsection{Convolutional Block Attention Module (CBAM)}
CBAM~\cite{woo2018cbam} sequentially applies channel and spatial attention.

\subsubsection{Channel Attention}
CBAM uses both average and max pooling:
\begin{equation}
\mathbf{M}_c = \sigma(MLP(AvgPool(\mathbf{X})) + MLP(MaxPool(\mathbf{X})))
\end{equation}

\subsubsection{Spatial Attention}
After channel refinement, spatial attention emphasizes informative regions:
\begin{equation}
\mathbf{M}_s = \sigma(f^{7\times7}([AvgPool(\mathbf{X'}); MaxPool(\mathbf{X'})]))
\end{equation}
where $[\cdot;\cdot]$ denotes channel-wise concatenation and $f^{7\times7}$ is a $7\times7$ convolution.

\section{Evaluation Metrics}

\subsection{Classification Accuracy}

Standard accuracy measures the proportion of correct predictions:

\begin{equation}
\text{Accuracy} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}}
\end{equation}

\subsection{Precision, Recall, and F1-Score}

For multi-class classification, we compute per-class metrics and aggregate using macro-averaging:

\begin{equation}
\text{Precision}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FP}_k}
\end{equation}

\begin{equation}
\text{Recall}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FN}_k}
\end{equation}

\begin{equation}
\text{F1}_k = 2 \cdot \frac{\text{Precision}_k \cdot \text{Recall}_k}{\text{Precision}_k + \text{Recall}_k}
\end{equation}

Macro-averaging treats all classes equally:

\begin{equation}
\text{Macro-F1} = \frac{1}{K} \sum_{k=1}^{K} \text{F1}_k
\end{equation}

\subsection{ROC-AUC}

The Area Under the Receiver Operating Characteristic curve (ROC-AUC) measures classification performance across all decision thresholds. For multi-class problems, we use one-vs-rest with macro-averaging:

\begin{equation}
\text{AUC}_{\text{macro}} = \frac{1}{K} \sum_{k=1}^{K} \text{AUC}_k
\end{equation}

ROC-AUC is particularly useful for imbalanced datasets as it is insensitive to class distribution.

\subsection{Top-K Accuracy}

Top-K accuracy measures whether the correct class appears in the model's top-K predictions:

\begin{equation}
\text{Top-K Acc} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[y_i \in \text{Top-K}(\hat{y}_i)]
\end{equation}

For audio classification, Top-3 accuracy is relevant as perceptually similar sounds may have ambiguous labels.

\subsection{Statistical Significance}

To ensure reproducibility and account for training stochasticity, we run experiments with multiple random seeds and report:

\begin{equation}
\mu \pm \sigma = \frac{1}{n}\sum_{i=1}^{n} x_i \pm \sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i - \mu)^2}
\end{equation}

This provides confidence intervals for performance comparisons between architectures.


%%%%%%%%%%%%%%%%%%%%%%
%%% Methodology
\chapter{Methodology}

This chapter describes the experimental methodology used to evaluate deep learning architectures for audio classification. We detail the datasets, model architectures, training procedures, and evaluation metrics employed in this study.

\section{Datasets}

We evaluate our models on two audio classification datasets with different characteristics: ESC-50 for environmental sound classification and EmoDB for speech emotion recognition.

\subsection{ESC-50: Environmental Sound Classification}

ESC-50 is a publicly available dataset consisting of 2,000 audio recordings, each 5 seconds in duration, divided into 50 semantic classes across 5 major categories (animals, natural soundscapes, human non-speech sounds, interior/domestic sounds, exterior/urban noises). Each class contains 40 examples recorded at 44.1 kHz in mono.

\subsection{EmoDB: Berlin Database of Emotional Speech}

EmoDB is a German emotional speech corpus containing 535 utterances produced by 10 professional actors (5 male, 5 female) expressing 7 emotions: anger (127), boredom (81), disgust (46), fear (69), happiness (71), sadness (62), and neutral (79). Recordings were made in an anechoic chamber at 48 kHz.

\subsection{Data Preprocessing}

\subsubsection{Spectrogram Generation}

Audio waveforms are converted to Mel-spectrograms using:
\begin{itemize}
    \item Sampling rate: 22,050 Hz
    \item FFT window: 2048 samples (Hann window)
    \item Hop length: 512 samples
    \item Mel filters: 128 bands (20 Hz to 11,025 Hz)
    \item Output resolution: 224×224 pixels
\end{itemize}

\subsubsection{Data Augmentation}

Each original clip generates 5 augmented variants using additive Gaussian noise, pitch shifting (±2 semitones), time stretching (0.9×-1.1×), gain variation (±3 dB), and room reverb simulation.

\subsubsection{Train/Validation Split}

We employ stratified random splits: ESC-50 (80\% train, 20\% validation) and EmoDB (80\% train, 20\% validation, speaker-independent). All augmented variants of a clip are grouped during splitting to prevent data leakage.

\section{Model Architectures}

\subsection{Baseline CNN}

A lightweight 5-layer convolutional network trained from scratch with 3 convolutional blocks (32→64→128 channels), 2 fully-connected layers, and dropout (p=0.5).

\subsection{AlexNet}

AlexNet pre-trained on ImageNet with modified classifier head (4096→4096→num\_classes). Fine-tuning strategy: freeze conv layers initially, train classifier, then unfreeze all.

\subsection{ResNet-50}

ResNet-50 with ImageNet pre-training, 50-layer residual architecture with bottleneck blocks. Modified final layer: 2048→num\_classes. Training strategy: freeze early layers, fine-tune layer4 + classifier.

\subsection{ResNet-50 with Attention Mechanisms}

\subsubsection{Squeeze-and-Excitation (SE)}

SE blocks inserted after each residual block with reduction ratio r=16. Channel-wise recalibration via global pooling→FC(C/16)→ReLU→FC(C)→Sigmoid. Minimal computational overhead (<2\% parameters).

\subsubsection{CBAM}

CBAM applies sequential channel and spatial attention. Channel: Max+Avg pooling→shared MLP. Spatial: Max+Avg pooling→7×7 conv→sigmoid. Inserted after layers 1-4 in ResNet-50.

\section{Training Configuration}

\subsection{Hyperparameters}

Consistent across all experiments:
\begin{itemize}
    \item Optimizer: AdamW
    \item Learning rate: 5×10$^{-4}$
    \item Weight decay: 1×10$^{-2}$
    \item Batch size: 64
    \item Epochs: 30
    \item Loss function: Cross-entropy
\end{itemize}

\subsection{Reproducibility}

Random seeds: 42, 123, 999. All experiments run with 3 seeds. Results report mean ± standard deviation.

\section{Evaluation Metrics}

\subsection{Classification Accuracy}

Standard top-1 accuracy: Accuracy = Correct Predictions / Total Predictions

\subsection{Macro-averaged F1 Score}

Treats all classes equally, addressing class imbalance.

\subsection{Top-3 Accuracy}

Measures whether correct class appears in top 3 predictions.

\subsection{Confusion Matrix}

Per-class confusion matrices identify systematic misclassification patterns.

%%%%%%%%%%%%%%%%%%%%%%
%%% Implementation
%%%%%%%%%%%%%%%%%%%%%%
%%% Implementation
\chapter{Implementation}

This chapter details the software implementation of the project, including the system architecture, code structure, and key technical challenges encountered during development.

\section{System Architecture}

The project is implemented in Python 3.12 using the PyTorch deep learning framework. The codebase follows a modular design to ensure extensibility and reproducibility.

\subsection{Directory Structure}

The project is organized into logical components:
\begin{itemize}
    \item \texttt{product/models/}: Contains model definitions (CBAM, SE blocks).
    \item \texttt{product/training/}: Training scripts for each architecture (Baseline, AlexNet, ResNet).
    \item \texttt{product/artifacts/}: Stores datasets, splits, and experiment results.
    \item \texttt{notebooks/}: Jupyter notebooks for data exploration and visualization.
\end{itemize}

\subsection{System Architecture Overview}

Figure~\ref{fig:system_architecture} illustrates the complete system architecture, showing the data pipeline, model variants, and training/evaluation workflow.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{system_architecture.jpg}
\caption{System architecture showing the end-to-end pipeline from audio files to trained models and evaluation metrics.}
\label{fig:system_architecture}
\end{figure}

The architecture follows a modular design where each component (data preprocessing, model training, evaluation) is independently testable and reusable across different experiments.

\section{Data Pipeline}

Efficient data loading is critical for training deep learning models. We implemented a custom \texttt{SpectrogramCSVDataset} class inheriting from PyTorch's \texttt{Dataset}.

\subsection{On-the-Fly Loading}
Instead of loading all spectrograms into memory, the dataset reads images on-demand from disk. This allows for scalability to larger datasets. The pipeline includes:
\begin{enumerate}
    \item Reading file paths and labels from a CSV manifest.
    \item Loading the image using PIL (Python Imaging Library).
    \item Converting to Tensor and normalizing with ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).
\end{enumerate}

\section{Training Infrastructure}

\subsection{Experiment Automation}
To ensure robust results, we developed automated batch scripts (\texttt{run\_all\_experiments.bat}) that:
\begin{itemize}
    \item Execute training runs sequentially for multiple models.
    \item Automatically cycle through random seeds (42, 123, 999).
    \item Log all outputs to unique directories timestamped for traceability.
\end{itemize}

\subsection{Logging and Monitoring}
We integrated TensorBoard for real-time monitoring of:
\begin{itemize}
    \item Training and validation loss curves.
    \item Accuracy progression.
    \item Confusion matrices generated at the end of training.
\end{itemize}

\section{Key Technical Challenges}

\subsection{Data Leakage Prevention}
A critical issue was identified early in development where data augmentation caused leakage between training and validation sets. Naively splitting the dataset after augmentation resulted in original and augmented versions of the same clip appearing in both sets, artificially inflating accuracy.

\textbf{Solution:} The solution implemented is a grouped splitting strategy in which the split is done on the unique source file IDs before augmentation. All augmented variants of a source file are then assigned to the same split as the original, ensuring strict separation.

\subsection{Attention Mechanism Integration}
Integrating CBAM into the pre-trained ResNet-50 architecture required careful handling of layer freezing. Initial experiments showed high variance because the added attention modules in early layers were inadvertently frozen while only the final layers were trained.

\textbf{Solution:} We adapted the fine-tuning strategy so that we are explicitly not freezing out all added attention
modules (CBAM/SE) in all the layers but preserving the pre-trained convolutional weights
frozen during the first phase. This ensured that the attention mechanisms had the chance to learn effective
feature recalibration.

\subsection{Cross-Platform Compatibility}
Developing through Windows means dealing with path separators and encoding issues (e.g. Unicode
characters in logs) that are different from the Linux environments. We standardized on using python's
\texttt{pathlib} to be robust about the use of paths and make certain all the file I/O used explicit UTF-8 Encoding.

\section{Version Control and Reproducibility}

\subsection{Git Version Control}
The entire project is maintained under Git version control with a structured branching strategy:
\begin{itemize}
    \item \texttt{main} branch: Stable, tested code
    \item \texttt{dev} branch: Active development
    \item Feature branches: Individual experiments and model implementations
\end{itemize}

Commit messages follow conventional commit standards, documenting changes to models, datasets, and experimental configurations. This ensures full traceability of all experiments and enables rollback to previous states if needed.

\subsection{Experiment Tracking}
Each experiment run is logged with:
\begin{itemize}
    \item Timestamp and unique experiment ID
    \item Model architecture and hyperparameters
    \item Random seed for reproducibility
    \item Training/validation metrics at each epoch
    \item Final confusion matrices and classification reports
\end{itemize}

All experiment artifacts are stored in timestamped directories under \texttt{product/artifacts/}, allowing comparison across runs.

\section{Testing and Validation}

\subsection{Data Integrity Testing}
Before training, we implemented validation checks to ensure:
\begin{itemize}
    \item No overlap between training and validation sets (grouped split verification)
    \item All spectrogram files are readable and correctly formatted
    \item Label distributions are balanced across splits
    \item Augmented variants are correctly linked to source files
\end{itemize}

\subsection{Model Validation}
To verify correct model implementation:
\begin{itemize}
    \item Loaded pre-trained weights and verified output dimensions match expected shapes
    \item Tested attention modules in isolation before integration
    \item Compared gradient flow through attention layers vs. baseline
    \item Validated that freezing/unfreezing strategies correctly update \texttt{requires\_grad} flags
\end{itemize}

\subsection{Multi-Seed Reproducibility}
All experiments are run with three random seeds (42, 123, 999) to quantify variance and ensure results are not artifacts of lucky initialization. This statistical rigor is essential for drawing valid conclusions about model performance.

\section{System Design Diagrams}

To illustrate the modular architecture and workflow of the system, we provide UML diagrams showing the class structure and training sequence.

\subsection{Class Diagram}

Figure~\ref{fig:uml_class} illustrates the modular class structure of the system. The \texttt{SpectrogramDataset} class handles data loading, while \texttt{BaseModel} provides an abstract interface for all model architectures. Specific models (\texttt{ResNet50Model}) inherit from this base, and attention modules (\texttt{SEBlock}, \texttt{CBAMBlock}) are implemented as reusable components. The \texttt{Trainer} class orchestrates the training process.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{uml_class_diagram.png}
\caption{UML class diagram showing the modular architecture with inheritance relationships and key components.}
\label{fig:uml_class}
\end{figure}

\subsection{Training Sequence Diagram}

Figure~\ref{fig:uml_sequence} shows the training sequence, depicting the interaction between components during a single training iteration. The \texttt{Trainer} coordinates data loading, forward pass, loss computation, backpropagation, and logging.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{uml_sequence_diagram.png}
\caption{UML sequence diagram illustrating the training pipeline flow from data loading to metric logging.}
\label{fig:uml_sequence}
\end{figure}

These diagrams demonstrate the separation of concerns and modularity of the codebase, facilitating maintainability and extensibility.

%%%%%%%%%%%%%%%%%%%%%%
%%% Results
\chapter{Results and Analysis}

\section{Experimental Results}

We present the quantitative results of our experiments on the ESC-50 and EmoDB datasets. Table~\ref{tab:esc50_results} and Table~\ref{tab:emodb_results} summarize the performance of the evaluated architectures.

\subsection{ESC-50 Results}

The experiments on the ESC-50 dataset demonstrate the clear advantage of transfer learning. The Baseline CNN, trained from scratch, achieved a validation accuracy of only 44.4\%. In contrast, the ImageNet pre-trained models performed significantly better.

AlexNet achieved an average accuracy of 74.5\%, while the deeper ResNet-50 model reached 89.9\%. The integration of Squeeze-and-Excitation (SE) blocks into ResNet-50 yielded an average accuracy of 88.8\%, which is statistically comparable to the vanilla ResNet-50. The CBAM-augmented model demonstrated significant instability, achieving a mean accuracy of 73.4\% with high variance ($\pm$10.9\%) across seeds.

\begin{table}[h]
\centering
\caption{Performance comparison on ESC-50 dataset (Validation Set).}
\label{tab:esc50_results}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Runs} & \textbf{Avg. Acc (\%)} & \textbf{Std Dev} & \textbf{Avg. Macro F1} \\ \hline
Baseline CNN & 1 & 44.4 & -- & 0.424 \\
AlexNet & 2 & 74.5 & $\pm$4.3 & 0.738 \\
ResNet-50 & 3 & 89.9 & $\pm$0.9 & 0.898 \\
ResNet-50 + SE & 3 & 88.8 & $\pm$2.4 & 0.887 \\
ResNet-50 + CBAM & 3 & 73.4 & $\pm$10.9 & -- \\ \hline
\end{tabular}
\end{table}

\subsection{EmoDB Results}

On the EmoDB dataset, both ResNet-50 and ResNet-50 + SE achieved very high performance, indicating that the models can effectively distinguish between emotional states from speech spectrograms.

\begin{table}[h]
\centering
\caption{Performance comparison on EmoDB dataset (Validation Set).}
\label{tab:emodb_results}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Runs} & \textbf{Avg. Acc (\%)} & \textbf{Std Dev} & \textbf{Avg. Macro F1} \\ \hline
ResNet-50 & 2 & 95.7 & $\pm$3.2 & 0.946 \\
ResNet-50 + SE & 3 & 95.8 & $\pm$0.9 & 0.949 \\
ResNet-50 + CBAM & 3 & 80.2 & $\pm$19.3 & -- \\ \hline
\end{tabular}
\end{table}

\section{Analysis}

The results highlight several key findings:
\begin{itemize}
    \item \textbf{Impact of Depth and Pre-training:} ResNet-50 outperforms AlexNet by a large margin ($\sim$15\%), confirming that deeper architectures with residual connections are better suited for capturing the complex time-frequency patterns in spectrograms.
    
    \item \textbf{Effectiveness of SE Attention:} While SE blocks show competitive performance (88.8\% vs 89.9\% baseline), the average accuracy is slightly lower with higher variance ($\pm$2.4\% vs $\pm$0.9\%). The best individual SE run achieved 91.6\%, suggesting SE can help in some initializations, but doesn't consistently improve over vanilla ResNet-50.
    
    \item \textbf{CBAM Instability:} CBAM demonstrated extreme training instability across both datasets. On ESC-50, performance ranged from 63.7\% to 85.1\% (mean: 73.4\% $\pm$10.9\%). On EmoDB, the variance was even more severe: 58.4\% to 95.3\% (mean: 80.2\% $\pm$19.3\%). This 37-point spread on EmoDB indicates that CBAM is highly sensitive to random initialization, with only 1 out of 3 seeds achieving competitive performance.
    
    \item \textbf{Why Attention Doesn't Help:} The limited benefit from attention mechanisms suggests that ResNet-50's learned features already capture the discriminative spectral-temporal patterns needed for classification. Channel recalibration (SE) provides minimal gains, while spatial attention (CBAM) actively harms performance, possibly due to overfitting or inappropriate inductive biases for audio data.
    
    \item \textbf{Generalization:} The high accuracy on EmoDB ($>$95\%) confirms that the transfer learning approach generalizes well to speech emotion recognition tasks. Interestingly, SE shows the same pattern here: competitive with baseline but with higher variance.
\end{itemize}

\subsection{Per-Class Performance Analysis}

To understand where the models succeed and fail, we analyzed the per-class performance on ESC-50 using the ResNet-50 baseline (seed 42, 89.4\% accuracy).

\textbf{Best Performing Classes (100\% recall):}
\begin{itemize}
    \item Class 6: Hen (perfect classification)
    \item Class 10: Chainsaw (perfect classification)
    \item Class 20: Crackling fire (perfect classification)
    \item Class 24: Helicopter (perfect classification)
    \item Class 35: Washing machine (perfect classification)
\end{itemize}

These classes likely have distinctive spectral signatures that are easily separable.

\textbf{Worst Performing Classes ($<$60\% recall):}
\begin{itemize}
    \item Class 29: 46.9\% recall (most difficult class)
    \item Class 28: 59.4\% recall
    \item Class 33: 59.4\% recall
\end{itemize}

Analysis of the confusion matrices reveals that these challenging classes often share acoustic similarities with other categories. For example, animal sounds (dog, cat, rooster) are frequently confused due to overlapping frequency characteristics in their vocalizations.

\subsection{Confusion Matrix Visualization}

Figures~\ref{fig:cm_resnet50} through~\ref{fig:cm_cbam} show the confusion matrices for different models on ESC-50. The diagonal elements represent correct classifications, while off-diagonal elements indicate misclassifications.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{cm_resnet50.png}
\caption{Confusion matrix for ResNet-50 baseline on ESC-50 (seed 42, 89.4\% accuracy). Strong diagonal indicates robust classification.}
\label{fig:cm_resnet50}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{cm_se.png}
\caption{Confusion matrix for ResNet-50+SE on ESC-50 (seed 42, 91.6\% accuracy). Pattern nearly identical to baseline.}
\label{fig:cm_se}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{cm_cbam.png}
\caption{Confusion matrix for ResNet-50+CBAM on ESC-50 (seed 42, 85.1\% accuracy). Weaker diagonal with increased scatter.}
\label{fig:cm_cbam}
\end{figure}

\textbf{Key Observations:}
\begin{enumerate}
    \item \textbf{ResNet-50 Baseline:} Strong diagonal with minimal off-diagonal scatter, indicating robust classification across most classes.
    \item \textbf{ResNet-50+SE:} Nearly identical pattern to baseline, confirming that SE attention provides marginal benefit.
    \item \textbf{ResNet-50+CBAM (Seed 42):} Weaker diagonal with increased off-diagonal scatter, particularly in the middle classes (15-35), indicating reduced discriminative power despite achieving 85\% accuracy.
\end{enumerate}

The confusion matrices visually confirm our quantitative findings: SE maintains baseline performance, while CBAM introduces classification errors even in its best-case scenario (seed 42).

\subsection{Cross-Dataset Comparison}

Comparing ESC-50 (50 classes, diverse environmental sounds) to EmoDB (7 classes, controlled speech):

\begin{itemize}
    \item \textbf{EmoDB} achieves higher accuracy ($>$95\%) due to:
    \begin{itemize}
        \item Fewer classes (7 vs 50)
        \item Controlled recording conditions
        \item Structured emotional prosody patterns
    \end{itemize}
    
    \item \textbf{ESC-50} is more challenging ($\sim$90\%) due to:
    \begin{itemize}
        \item High intra-class variability (e.g., different dog breeds)
        \item Diverse recording environments
        \item Acoustic overlap between categories
    \end{itemize}
\end{itemize}

This cross-dataset validation demonstrates that the models generalize well across different audio domains, but performance is heavily dependent on dataset complexity and class separability.


%%%%%%%%%%%%%%%%%%%%%%
%%% Planning
\chapter{Planning and Schedule}

\section{Original Project Plan}

As a stepwise process in Term 1, the project was initially designed to start with a literature review and preprocessing. Then baseline and transfer-learning experiments were to be performed, followed by attention mechanism integration and, lastly, analysis and reporting. This plan offered a high-level design but did not anticipate the work being so iterative and debugging-intensive in experimental deep learning.

\section{Actual Time Plan for the Project (Diary Based)}

This part shows the actual timeline of the execution, reconstructed from the project diary, and points out where work took longer, or where work overlapped, compared to the initial plan.

\subsection*{Phase 1: Preprocessing and Pipeline Setup}
\textit{Early October -- Late October}

\begin{itemize}
    \item ESR literature review and ESC-50 data format review.
    \item Learning Librosa and audio feature extraction.
    \item Mel and Log-Mel spectrogram pipelines were developed.
    \item Data augmentation (noise, pitch shift, time stretch) implementation.
    \item Environment and dependency problem resolution.
    \item Reproducible preprocessing pipeline ($\sim$8k spectrograms) finalised.
\end{itemize}

\textbf{Note:} More time was spent here than initially planned because of environment instability and the need to validate preprocessing outputs.

\subsection*{Phase 2: CNN Baseline and Data Validation}
\textit{Late October -- Mid November}

\begin{itemize}
    \item Implementation and training of baseline CNN.
    \item Integration of logging, checkpointing, and evaluation metrics.
    \item Identification of data leakage due to augmented samples crossing splits.
    \item Redesign of the dataset splitting strategy in order to group by audio clip.
    \item Retraining of baseline model on corrected splits.
\end{itemize}

\textbf{Note:} Fixing data leakage became a necessary prerequisite before continuing with the rest.

\subsection*{Phase 3: Transfer Learning Models}
\textit{Mid November}

\begin{itemize}
    \item Transfer learning implementation of AlexNet.
    \item Transfer learning implementation of ResNet-50.
    \item Statistically stable multi-seed training (three seeds per model).
    \item Initial per-class and variance analysis.
    \item Optimisation of reproducibility setup (fixed seeds, consistent configs).
\end{itemize}

\subsection*{Phase 4: Attention Mechanisms}
\textit{End of November -- Early December}

\begin{itemize}
    \item Insertion of SE attention in ResNet-50.
    \item Training and assessment on different seeds.
    \item CBAM implementation and debugging.
    \item A single CBAM run was completed; others were terminated due to instability.
    \item Comparison with non-attention controls.
\end{itemize}

\textbf{Observation:} Attention mechanisms were not presented as a completely isolated step, but were introduced after establishing a strong baseline.

\subsection*{Phase 5: Experiments from Secondary Datasets (EmoDB)}
\textit{Late November}

\begin{itemize}
    \item Integration of EmoDB dataset.
    \item Preprocessing and splitting pipeline adaptation.
    \item Evaluation with ResNet-50 and SE-augmented models.
\end{itemize}

\textbf{Observation:} higher accuracy but higher variance compared to ESC-50.

\subsection*{Phase 6: Analysis and Interim Reporting}
\textit{Early December -- Mid December}

\begin{itemize}
    \item Aggregation of multi-seed results.
    \item Generation of confusion matrices and per-class analyses.
    \item Preparation of tables and figures for the interim report.
    \item Writing and refinement of the interim submission alongside final experiments.
\end{itemize}

\section{Summary}

While the original plan assumed a linear progression, in reality there was overlapping experimentation, debugging, and analysis. The adjustments to the timeline mostly occurred to ensure data integrity, reproducibility, and stable baselines before extending the work using attention mechanisms and additional datasets.

%%%%%%%%%%%%%%%%%%%%%%
%%% Diary
\chapter{Project Diary}

This chapter entails the time history of project implementation, in which the planned progress, as well as the deviations realised during implementation and experimentation, are recorded. The diary also indicates the repetitive and discovery-driven aspect of machine learning research, such as debugging, re-design choices, and partial or discontinued experimental directions where necessary.

\section{September 2025}
\subsection*{Week 1: Project Introduction and Administration}

\begin{itemize}
    \item Project scope / hi-tech direction confirmed with supervisor.
    \item Concurring core objective: comparative assessment of baseline CNNs and transfer learning architectures in audio classification.
    \item First repository structure created (product/, documents/, .gitignore).
    \item Institutionalised Git-based version control and reproducibility.
\end{itemize}

\subsection*{Week 2: Preliminary Research on the Background}

\begin{itemize}
    \item Summarised background information on neural networks applicable to CNN-based classification.
    \item Learned concepts and spectrogram representations of audio signal processing.
    \item Conducted a literature review of earlier studies regarding environmental sound recognition (ESR).
    \item Researched Librosa, waveform loading, transformation, and visualisation.
\end{itemize}

\section{October 2025}
\subsection*{Week 1: Data Checking and Orienting}

\begin{itemize}
    \item ESC-50 downloaded and integrity of the data checked (2000 clips, 50 classes).
    \item Metadata compared with audio files.
    \item Produced verification notebooks for inspection of waveforms and spectrograms.
    \item Preprocessing: summarised major Librosa utilities to be used later in the pipeline.
\end{itemize}

\subsection*{Weeks 2--3: Preprocessing Pipeline Development}

\begin{itemize}
    \item Applied scripts for waveform visualisation and Mel-spectrogram generation.
    \item Disturbances in the Python environment (problems with Numba/LLVM) were fixed.
    \item Moved to a new clean virtual environment in an attempt to stabilise the setup.
    \item Refactored prepared preprocessing code into a modular structure which can be re-used.
\end{itemize}

\subsection*{Week 4: Augmentation and CNN Preliminary Baseline}

\begin{itemize}
    \item Completed audio augmentation pipeline (noise injection, pitch changes, time stretch).
    \item Installed a preliminary CNN model.
    \item Authentication of input shapes and forward-pass integrity.
    \item Native TensorBoard logging used to track training dynamics.
\end{itemize}

\section{November 2025}
\subsection*{Weeks 1--2: Preparation and Expansion of Data}

\begin{itemize}
    \item Mel-spectrograms for ESC-50 generated.
    \item Prepared augmented data (approximately 7,900 spectrograms).
    \item Initial train/validation splits implemented.
    \item Initial early CNN training runs conducted.
\end{itemize}

\subsection*{Week 3: CNN Base Assessment and Limitations}

\begin{itemize}
    \item Fine-tuned base CNN to convergence.
    \item Obtained validation accuracy of approximately 44\% on ESC-50.
    \item Performance ceiling identified with little generalisation.
    \item Decided that transfer learning was necessary in order to obtain better results.
\end{itemize}

\subsection*{Week 4: Transfer Learning and Critical Bug Discovery}

\begin{itemize}
    \item AlexNet implemented with ImageNet pretraining.
    \item ESC-50 validation accuracy of approximately 74\% achieved.
    \item Transfer-learning baseline of ResNet-50 implemented.
    \item Initial test performances were very high (~89\%).
    \item Major problem identified: data leakage caused by augmentation performed prior to dataset splitting.
    \item Both training and validation sets contained augmented versions of the same audio clip.
\end{itemize}

\subsection*{Week 4 (Follow-up): Data Leakage Remediation and Re-runs}

\begin{itemize}
    \item Recoded dataset splitting algorithm to split by original clip ID.
    \item Enforced split-before-augmentation policy.
    \item Regenerated datasets and re-trained baseline and transfer-learning models.
    \item Established corrected performance using lower yet accurate measures.
\end{itemize}

\subsection*{Week 5: Mechanisms of Attention (Exploratory Phase)}

\begin{itemize}
    \item Integrated Squeeze-and-Excitation (SE) blocks into ResNet-50.
    \item Conducted multi-seed experiments (42, 123, 999).
    \item Noticed close performance to baseline ResNet-50 with low variance.
    \item Added CBAM attention modules to ResNet-50.
    \item Initial CBAM experiments were very unstable across seeds.
    \item Attention work was considered exploratory due to reproducibility issues.
\end{itemize}

\subsection*{Week 6: EmoDB Data Mining}

\begin{itemize}
    \item EmoDB dataset (535 utterances, 7 emotions) downloaded and inspected.
    \item Introduced speaker-independent train/validation splits.
    \item Obtained approximately 3,200 augmented spectrograms.
    \item Assessed ResNet-50 and ResNet-50+SE.
    \item High accuracy observed with higher variance compared to ESC-50.
\end{itemize}

\section{December 2025}
\subsection*{Week 1: Debugging, Consolidation, and Final Experiments}

\begin{itemize}
    \item CBAM instability investigated in detail.
    \item Determined frozen attention parameters in the initial ResNet blocks.
    \item Adjusted layer unfreezing strategy.
    \item Repeated a limited number of experiments with CBAM.
    \item Concluded that complete multi-seed CBAM reproducibility was not achievable within the available timeframe.
    \item Emphasised stability and correctness of baseline and SE outcomes.
\end{itemize}

\subsection*{Week 2: Analysis, Writing, and Presentation Preparation}

\begin{itemize}
    \item Combined trustworthy multi-model results.
    \item Created confusion matrices and per-class performance analyses.
    \item Automated LaTeX table creation for results reporting.
    \item Composed interim report chapters (Introduction, Methodology, Implementation, Results).
    \item Prepared presentation slides containing results and encountered obstacles.
    \item By the end of the project period (December 6--12, 2025), finalised materials for interim presentation.
\end{itemize}

\section*{Closing Note}

This diary reflects a non-linear research process, where time was redistributed towards debugging, data integrity, and reproducibility. Several experimental directions (especially CBAM attention) were explored but intentionally limited to ensure the validity and reliability of the reported results.


%%%% BIBLIOGRAPHY
\newpage
\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography}

\bibitem{piczak2015esc} Karol J. Piczak. \emph{ESC: Dataset for Environmental Sound Classification}. In Proceedings of the 23rd ACM International Conference on Multimedia, 2015.

\bibitem{burkhardt2005emodb} Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F. Sendlmeier, and Benjamin Weiss. \emph{A Database of German Emotional Speech}. In Proceedings of Interspeech, 2005.

\bibitem{stevens1937scale} Stanley Smith Stevens, John Volkmann, and Edwin B. Newman. \emph{A Scale for the Measurement of the Psychological Magnitude Pitch}. The Journal of the Acoustical Society of America, 1937.

\bibitem{lecun1998gradient} Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. \emph{Gradient-Based Learning Applied to Document Recognition}. Proceedings of the IEEE, 1998.

\bibitem{ioffe2015batch} Sergey Ioffe and Christian Szegedy. \emph{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}. In Proceedings of ICML, 2015.

\bibitem{krizhevsky2012imagenet} Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. \emph{ImageNet Classification with Deep Convolutional Neural Networks}. In Advances in Neural Information Processing Systems (NIPS), 2012.

\bibitem{he2016deep} Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \emph{Deep Residual Learning for Image Recognition}. In Proceedings of CVPR, 2016.

\bibitem{deng2009imagenet} Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \emph{ImageNet: A Large-Scale Hierarchical Image Database}. In Proceedings of CVPR, 2009.

\bibitem{hershey2017cnn} Shawn Hershey et al. \emph{CNN Architectures for Large-Scale Audio Classification}. In Proceedings of ICASSP, 2017.

\bibitem{hu2018squeeze} Jie Hu, Li Shen, and Gang Sun. \emph{Squeeze-and-Excitation Networks}. In Proceedings of CVPR, 2018.

\bibitem{woo2018cbam} Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. \emph{CBAM: Convolutional Block Attention Module}. In Proceedings of ECCV, 2018.

\bibitem{gemmeke2017audioset} Jort F. Gemmeke et al. \emph{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}. In Proceedings of ICASSP, 2017.

\bibitem{kong2020panns} Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley. \emph{PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition}. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020.

\bibitem{gong2021ast} Yuan Gong, Yu-An Chung, and James Glass. \emph{AST: Audio Spectrogram Transformer}. In Proceedings of Interspeech, 2021.

\bibitem{purwins2019deep} Hendrik Purwins, Bo Li, Tuomas Virtanen, Jan Schlüter, Shuo-Yiin Chang, and Tara Sainath. \emph{Deep Learning for Audio Signal Processing}. IEEE Journal of Selected Topics in Signal Processing, 2019.

\bibitem{zhang2021attention} Zhichao Zhang, Shugong Xu, Shunqing Zhang, Shan Cao, and Jia Liu. \emph{Attention Based Convolutional Recurrent Neural Network for Environmental Sound Classification}. Neurocomputing, 2021.

\bibitem{salamon2017deep} Justin Salamon and Juan Pablo Bello. \emph{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}. IEEE Signal Processing Letters, 2017.

\bibitem{yosinski2014transferable} Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. \emph{How Transferable are Features in Deep Neural Networks?}. In Advances in Neural Information Processing Systems (NIPS), 2014.

\bibitem{mcfee2015librosa} Brian McFee, Colin Raffel, Dawen Liang, Daniel P.W. Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. \emph{librosa: Audio and Music Signal Analysis in Python}. In Proceedings of the 14th Python in Science Conference, 2015.

\end{thebibliography}
\label{endpage}

\end{document}
