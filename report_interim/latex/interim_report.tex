% !TEX root = interim_report.tex
\documentclass[11pt]{report}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

% --- START OF CLASS FILE CONTENT ---
\makeatletter
\usepackage[paperwidth=21cm,paperheight=29.7cm,includehead,headheight=1.5cm,pdftex,hmargin={3cm,2.5cm},vmargin={0cm,2cm},]{geometry} 
\usepackage{fancyhdr}
\setlength{\headheight}{1.5cm}
\setlength{\parindent}{0cm} 
\renewcommand{\baselinestretch}{1.2}
\parskip=1em


\renewcommand{\contentsname}{Table of Contents}

\def\ps@plain{%
      \let\@oddhead\@empty
      \def\@oddfoot{\normalfont\hfil\small\textsf{Page \thepage~of~\pageref{endpage}}\hfil}%
      \def\@evenfoot{\normalfont\hfil\small\textsf{Page \thepage~of~\pageref{endpage}}\hfil}}


\renewcommand{\normalsize}{\fontsize{11pt}{11pt}\selectfont}
\renewcommand{\title}[1]{{\sffamily\Huge #1\par}}
\renewcommand{\author}[1]{{\sffamily\Huge #1\par}}
\newcommand{\subtitle}[1]{\textsf{\textbf{\Large{#1}}}}
\newcommand{\abstractheading}[1]{\textsf{\textbf{\LARGE{#1}}}}
\newcommand{\code}[1]{\texttt{\footnotesize{#1}}}

\renewcommand{\section}{\@startsection
{section}%                    % the name
{1}%                          % the level
{0mm}%                        % the indent
{6mm}%             % the beforeskip
{4.2mm}%           % the afterskip
{\LARGE\bfseries\sffamily}}  % the style

\renewcommand{\subsection}{\@startsection
{subsection}%                    % the name
{2}%                          % the level
{0mm}%                        % the indent
{4mm}%             % the beforeskip
{1.1mm}%           % the afterskip
{\Large\bfseries\sffamily}}  % the style

\renewcommand{\subsubsection}{\@startsection
{subsubsection}%                    % the name
{3}%                          % the level
{0mm}%                        % the indent
{4.2mm}%             % the beforeskip
{1.1mm}%           % the afterskip
{\normalsize\bfseries\sffamily}}  % the style

\renewcommand\chapter{
\if@openright\cleardoublepage\else\clearpage\fi
                    \thispagestyle{plain}%
                    \global\@topnum\z@
                    \@afterindentfalse
                    \secdef\@chapter\@schapter}

%% Chapter headings should be at the top of the page.
\def\@makechapterhead#1{%
  { \parindent \z@ \raggedright \normalfont
    %\centering
    \ifnum \c@secnumdepth >\m@ne
        \huge\textsf{\@chapapp\space \thechapter:}
        % \par\nobreak
        %\vskip 20\p@
    \fi
    \interlinepenalty\@M
    \huge \bfseries \textsf{#1}\par\nobreak
%    \rule{5cm}{0.5pt}
    \vskip 20\p@
  } }
  
\def\@makeschapterhead#1{%
  %\vspace*{50\p@}%
  { \parindent \z@ \raggedright
    %\centering
    \normalfont
    \interlinepenalty\@M
    \huge \bfseries  \textsf{#1}\par\nobreak
%    \rule{5cm}{0.5pt}
    \vskip 20\p@

  }}
  
 \renewenvironment{abstract}{%
      \chapter*{\abstractname}%
      \addcontentsline{toc}{chapter}{\abstractname}
 }
     
     
\makeatletter
\renewcommand{\l@chapter}{\bfseries\@dottedtocline{1}{0em}{2.3em}}
\renewcommand{\l@section}{\normalfont\@dottedtocline{2}{2em}{2.3em}}
\renewcommand{\l@subsection}{\normalfont\@dottedtocline{3}{2em}{2.3em}}
\renewcommand{\l@subsubsection}{\normalfont\@dottedtocline{4}{2em}{2.3em}}
\makeatother

\def\maketitle{\begin{titlepage}
%\thispagestyle{empty}
\pagenumbering{gobble}
\thispagestyle{myheadings}
\markright{ \hfill \textnormal{\footnotesize{\studentname, \reportyear}} \hfill }
\let\footnotesize\small \let\footnoterule\relax \setcounter{page}{0}
\null
\vfil
\begin{center}
\title{Final Year Project Report}\vspace{0.25cm}
\Large \textbf{\fullOrHalfUnit~-~\finalOrInterim}\\[0.5cm]\rule{4cm}{1pt}\\[0.7cm]
\title{\textbf{\projecttitle}}\vspace{1cm}
\author{\LARGE \studentname}\vspace{0.5cm}\rule{4cm}{1pt}\\[0.5cm]
\textsf{\Large  A report submitted in part fulfilment of the degree of\\[0.5cm]
\textbf{\degree}}\\[0.5cm]
\textsf{\Large  \textbf{Supervisor:}  \supervisorname}\\[2cm]
\includegraphics[height=5cm]{logo}\\[2cm]
\textsf{\Large Department of Computer Science\\
Royal Holloway, University of London\\\vfill
\normalsize \today}
\end{center}
\vfil
\null
\end{titlepage}
\pagenumbering{arabic}
}

\pagestyle{fancyplain}
\lhead{\footnotesize \shortprojecttitle}
\rhead{\footnotesize \studentname}
\renewcommand{\headrulewidth}{0pt}

\usepackage{tocloft}
\renewcommand{\cftchapfont}{\sffamily}
\renewcommand{\cftsecfont}{\sffamily}
\renewcommand{\cftchappagefont}{\sffamily}
\renewcommand{\cftsecpagefont}{\sffamily}
\renewcommand{\cftchapleader}{\sffamily\cftdotfill{\cftsecdotsep}}
\setlength{\cftbeforesecskip}{\cftbeforechapskip}
\setcounter{tocdepth}{1}
\makeatother
% --- END OF CLASS FILE CONTENT ---


%%%%%%%%%%%%%%%%%%%%%%
%%% Input project details
\def\studentname{Devansh Dev}
\def\reportyear{2024/2025}
\def\projecttitle{Comparative Evaluation of Deep Learning Architectures for Audio and Visual Recognition Tasks}
\def\shortprojecttitle{Comparative Evaluation of DL Architectures}
\def\supervisorname{Dr. Zhang}
\def\degree{BSc (Hons) in Computer Science}
\def\fullOrHalfUnit{Full Unit}
\def\finalOrInterim{Interim Report}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%
%%% Declaration

\chapter*{Declaration}

This report has been prepared on the basis of my own work. Where other published and unpublished source materials have been used, these have been acknowledged.

\vskip3em

Word Count: [To be completed]

\vskip3em

Student Name: \studentname

\vskip3em

Date of Submission: December 2024

\vskip3em

Signature:

\newpage

%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
\tableofcontents\pdfbookmark[0]{Table of Contents}{toc}\newpage

%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract

\begin{abstract}

This interim report presents a comparative evaluation of deep learning architectures for audio classification tasks, focusing on the efficacy of attention mechanisms in Convolutional Neural Networks (CNNs). We investigate whether integrating Squeeze-and-Excitation (SE) and Convolutional Block Attention Modules (CBAM) into a ResNet-50 backbone enhances performance compared to standard baselines (AlexNet, ResNet-50) on environmental sound (ESC-50) and speech emotion (EmoDB) datasets. Our methodology employs transfer learning from ImageNet, converting audio signals into Mel-spectrograms. Preliminary results indicate that while transfer learning provides a significant performance boost over simpler architectures (ResNet-50 achieving $\sim$90\% on ESC-50 vs. 74\% for AlexNet), the addition of SE attention yields competitive results (88.8\%). This suggests that standard CNN features may already capture the necessary discriminative information for these tasks. We also validate the generalization capability of these models on the EmoDB dataset, achieving high accuracy ($>$95\%). The report details the experimental framework, technical challenges such as data leakage prevention, and outlines the remaining work for the final dissertation.

\end{abstract}
\newpage

%%%%%%%%%%%%%%%%%%%%%%
%%% Project Specification

\chapter*{Project Specification}
\addcontentsline{toc}{chapter}{Project Specification}

[To be completed]

%%%%%%%%%%%%%%%%%%%%%%
%%% Introduction
\chapter{Introduction}

[To be completed]

%%%%%%%%%%%%%%%%%%%%%%
%%% Background Theory
\chapter{Background Theory}
\label{ch:background}

This chapter provides the theoretical foundations for understanding deep learning architectures applied to audio classification tasks.

\section{Audio Representation and Spectrograms}

\subsection{Short-Time Fourier Transform}

Audio signals are one-dimensional time-series data. To capture both temporal and spectral information simultaneously, we transform audio into spectrograms using the Short-Time Fourier Transform (STFT):

\begin{equation}
X(m, \omega) = \sum_{n=-\infty}^{\infty} x[n] w[n - m] e^{-j\omega n}
\end{equation}

where $x[n]$ is the discrete audio signal, $w[n]$ is a window function (typically Hann window), $m$ is the time frame index, and $\omega$ is the angular frequency. The STFT provides a time-frequency representation by applying the Fourier transform to short, overlapping segments of the signal.

\subsection{Mel-Frequency Spectrograms}

Human auditory perception of frequency is approximately logarithmic rather than linear. The mel scale~\cite{stevens1937scale} provides a perceptually-motivated frequency representation:

\begin{equation}
m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)
\end{equation}

where $f$ is frequency in Hz and $m$ is frequency in mels. We apply triangular mel-scale filterbanks to the STFT magnitude spectrum to obtain mel-spectrograms, which are then converted to logarithmic scale (decibels) for improved dynamic range:

\begin{equation}
S_{dB} = 10 \log_{10}(S_{mel} + \epsilon)
\end{equation}

where $\epsilon$ is a small constant (typically $10^{-10}$) to avoid numerical instability from $\log(0)$.

This transformation produces a 2D representation where the horizontal axis represents time, the vertical axis represents mel-frequency bins, and pixel intensity represents energy. This visual representation allows us to apply computer vision techniques to audio classification tasks.

\section{Convolutional Neural Networks}

\subsection{Convolution Operation}

Convolutional Neural Networks (CNNs)~\cite{lecun1998gradient} apply learnable filters to extract hierarchical features from input data. For a 2D input $I$ and filter kernel $K$, the discrete convolution operation is defined as:

\begin{equation}
(I * K)(i, j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m, n)
\end{equation}

For spectrograms, convolution captures local time-frequency patterns. Early convolutional layers typically detect simple patterns such as edges and textures, while deeper layers learn increasingly complex combinations such as harmonic structures and temporal sequences.

\subsection{Pooling Operations}

Pooling layers reduce spatial dimensions while retaining important features. Max pooling selects the maximum value in each local region:

\begin{equation}
y_{i,j} = \max_{(m,n) \in R_{i,j}} x_{m,n}
\end{equation}

where $R_{i,j}$ is the pooling region. Average pooling computes the mean instead. Pooling provides translation invariance and reduces computational cost.

\subsection{Batch Normalization}

Batch Normalization~\cite{ioffe2015batch} normalizes layer inputs to stabilize training and enable higher learning rates:

\begin{equation}
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
\end{equation}

\begin{equation}
y = \gamma \hat{x} + \beta
\end{equation}

where $\mu_B$ and $\sigma_B^2$ are the batch mean and variance, and $\gamma$ and $\beta$ are learnable scale and shift parameters.

\subsection{Activation Functions}

Non-linear activation functions enable neural networks to learn complex patterns. The Rectified Linear Unit (ReLU) is widely used:

\begin{equation}
\text{ReLU}(x) = \max(0, x)
\end{equation}

ReLU addresses the vanishing gradient problem and provides computational efficiency compared to sigmoid or tanh activations.

\section{Deep Learning Architectures}

\subsection{AlexNet}

AlexNet~\cite{krizhevsky2012imagenet} demonstrated the effectiveness of deep CNNs for image classification, winning the ImageNet 2012 competition. The architecture consists of five convolutional layers followed by three fully-connected layers, with ReLU activations and dropout for regularization.

Key innovations include:
\begin{itemize}
    \item Use of ReLU activation instead of tanh
    \item Overlapping max pooling
    \item Dropout regularization (p=0.5)
    \item Data augmentation
\end{itemize}

\subsection{ResNet-50}

Residual Networks (ResNet)~\cite{he2016deep} introduced skip connections to address the degradation problem in very deep networks. The key innovation is the residual block:

\begin{equation}
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}
\end{equation}

where $\mathcal{F}(\mathbf{x}, \{W_i\})$ represents the residual mapping to be learned, and the identity mapping $\mathbf{x}$ is added via a skip connection. This formulation allows gradients to flow directly through the network, enabling training of networks with 50, 101, or even 152 layers.

ResNet-50 consists of 50 layers organized into four stages, each containing multiple residual blocks. The architecture uses bottleneck blocks with 1×1, 3×3, and 1×1 convolutions to reduce computational cost while maintaining representational power.

\section{Transfer Learning}

\subsection{Pre-training and Fine-tuning}

Transfer learning leverages knowledge learned from large-scale datasets (e.g., ImageNet~\cite{deng2009imagenet} with 1.2M images and 1000 classes) to improve performance on target tasks with limited data. Despite the domain difference between natural images and spectrograms, low-level features (edges, textures) and mid-level features (patterns, shapes) transfer effectively.

The transfer learning process typically involves two phases:

\textbf{Phase 1 - Feature Extraction (Linear Probing):} Freeze all convolutional layers and train only the final classification layer. This adapts the pre-trained feature representations to the new task without catastrophic forgetting of learned features.

\textbf{Phase 2 - Fine-tuning:} Unfreeze some or all layers and continue training with a lower learning rate. This allows task-specific adaptation of features while preserving useful representations from pre-training.

\subsection{Domain Adaptation}

Applying models pre-trained on natural images to audio spectrograms represents cross-domain transfer learning. While spectrograms are visual representations, they differ from natural images in several ways:
\begin{itemize}
    \item Spectrograms have structured axes (time and frequency) with physical meaning
    \item Patterns are often more regular and repetitive
    \item Color information is absent (spectrograms are grayscale)
\end{itemize}

Despite these differences, empirical evidence shows that ImageNet pre-training provides a strong initialization for spectrogram-based audio classification~\cite{hershey2017cnn}.

\section{Attention Mechanisms}

Attention mechanisms enable neural networks to focus on informative regions while suppressing irrelevant information. This section describes two prominent attention mechanisms applicable to CNNs.

\subsection{Squeeze-and-Excitation (SE) Blocks}

SE blocks~\cite{hu2018squeeze} perform channel-wise attention by explicitly modeling interdependencies between channels.

\subsubsection{Architecture}

Given input feature map $\mathbf{X} \in \mathbb{R}^{H \times W \times C}$, SE blocks apply three operations:

\textbf{1. Squeeze:} Global average pooling aggregates spatial information:

\begin{equation}
z_c = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} x_c(i, j)
\end{equation}

This produces a channel descriptor $\mathbf{z} \in \mathbb{R}^C$.

\textbf{2. Excitation:} A two-layer fully-connected network with bottleneck learns channel dependencies:

\begin{equation}
\mathbf{s} = \sigma(W_2 \cdot \delta(W_1 \cdot \mathbf{z}))
\end{equation}

where $W_1 \in \mathbb{R}^{\frac{C}{r} \times C}$, $W_2 \in \mathbb{R}^{C \times \frac{C}{r}}$, $r$ is the reduction ratio (typically 16), $\delta$ is ReLU, and $\sigma$ is sigmoid.

\textbf{3. Scale:} Channel-wise multiplication applies learned weights:

\begin{equation}
\tilde{x}_c = s_c \cdot x_c
\end{equation}

\subsection{Convolutional Block Attention Module (CBAM)}

CBAM~\cite{woo2018cbam} extends channel attention by adding spatial attention, applying both sequentially.

\subsubsection{Channel Attention}

CBAM uses both average and max pooling:

\begin{equation}
\mathbf{M}_c = \sigma(MLP(AvgPool(\mathbf{X})) + MLP(MaxPool(\mathbf{X})))
\end{equation}

\subsubsection{Spatial Attention}

After channel refinement, spatial attention emphasizes informative regions:

\begin{equation}
\mathbf{M}_s = \sigma(f^{7\times7}([AvgPool(\mathbf{X'}); MaxPool(\mathbf{X'})]))
\end{equation}

where $[\cdot;\cdot]$ denotes channel-wise concatenation and $f^{7\times7}$ is a $7\times7$ convolution.

\section{Evaluation Metrics}

\subsection{Classification Accuracy}

Standard accuracy measures the proportion of correct predictions:

\begin{equation}
\text{Accuracy} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}}
\end{equation}

\subsection{Precision, Recall, and F1-Score}

For multi-class classification, we compute per-class metrics and aggregate using macro-averaging:

\begin{equation}
\text{Precision}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FP}_k}
\end{equation}

\begin{equation}
\text{Recall}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FN}_k}
\end{equation}

\begin{equation}
\text{F1}_k = 2 \cdot \frac{\text{Precision}_k \cdot \text{Recall}_k}{\text{Precision}_k + \text{Recall}_k}
\end{equation}

Macro-averaging treats all classes equally:

\begin{equation}
\text{Macro-F1} = \frac{1}{K} \sum_{k=1}^{K} \text{F1}_k
\end{equation}

\subsection{ROC-AUC}

The Area Under the Receiver Operating Characteristic curve (ROC-AUC) measures classification performance across all decision thresholds. For multi-class problems, we use one-vs-rest with macro-averaging:

\begin{equation}
\text{AUC}_{\text{macro}} = \frac{1}{K} \sum_{k=1}^{K} \text{AUC}_k
\end{equation}

ROC-AUC is particularly useful for imbalanced datasets as it is insensitive to class distribution.

\subsection{Top-K Accuracy}

Top-K accuracy measures whether the correct class appears in the model's top-K predictions:

\begin{equation}
\text{Top-K Acc} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[y_i \in \text{Top-K}(\hat{y}_i)]
\end{equation}

For audio classification, Top-3 accuracy is relevant as perceptually similar sounds may have ambiguous labels.

\subsection{Statistical Significance}

To ensure reproducibility and account for training stochasticity, we run experiments with multiple random seeds and report:

\begin{equation}
\mu \pm \sigma = \frac{1}{n}\sum_{i=1}^{n} x_i \pm \sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i - \mu)^2}
\end{equation}

This provides confidence intervals for performance comparisons between architectures.


%%%%%%%%%%%%%%%%%%%%%%
%%% Methodology
\chapter{Methodology}

This chapter describes the experimental methodology used to evaluate deep learning architectures for audio classification. We detail the datasets, model architectures, training procedures, and evaluation metrics employed in this study.

\section{Datasets}

We evaluate our models on two audio classification datasets with different characteristics: ESC-50 for environmental sound classification and EmoDB for speech emotion recognition.

\subsection{ESC-50: Environmental Sound Classification}

ESC-50 is a publicly available dataset consisting of 2,000 audio recordings, each 5 seconds in duration, divided into 50 semantic classes across 5 major categories (animals, natural soundscapes, human non-speech sounds, interior/domestic sounds, exterior/urban noises). Each class contains 40 examples recorded at 44.1 kHz in mono.

\subsection{EmoDB: Berlin Database of Emotional Speech}

EmoDB is a German emotional speech corpus containing 535 utterances produced by 10 professional actors (5 male, 5 female) expressing 7 emotions: anger (127), boredom (81), disgust (46), fear (69), happiness (71), sadness (62), and neutral (79). Recordings were made in an anechoic chamber at 48 kHz.

\subsection{Data Preprocessing}

\subsubsection{Spectrogram Generation}

Audio waveforms are converted to Mel-spectrograms using:
\begin{itemize}
    \item Sampling rate: 22,050 Hz
    \item FFT window: 2048 samples (Hann window)
    \item Hop length: 512 samples
    \item Mel filters: 128 bands (20 Hz to 11,025 Hz)
    \item Output resolution: 224×224 pixels
\end{itemize}

\subsubsection{Data Augmentation}

Each original clip generates 5 augmented variants using additive Gaussian noise, pitch shifting (±2 semitones), time stretching (0.9×-1.1×), gain variation (±3 dB), and room reverb simulation.

\subsubsection{Train/Validation Split}

We employ stratified random splits: ESC-50 (80\% train, 20\% validation) and EmoDB (80\% train, 20\% validation, speaker-independent). All augmented variants of a clip are grouped during splitting to prevent data leakage.

\section{Model Architectures}

\subsection{Baseline CNN}

A lightweight 5-layer convolutional network trained from scratch with 3 convolutional blocks (32→64→128 channels), 2 fully-connected layers, and dropout (p=0.5).

\subsection{AlexNet}

AlexNet pre-trained on ImageNet with modified classifier head (4096→4096→num\_classes). Fine-tuning strategy: freeze conv layers initially, train classifier, then unfreeze all.

\subsection{ResNet-50}

ResNet-50 with ImageNet pre-training, 50-layer residual architecture with bottleneck blocks. Modified final layer: 2048→num\_classes. Training strategy: freeze early layers, fine-tune layer4 + classifier.

\subsection{ResNet-50 with Attention Mechanisms}

\subsubsection{Squeeze-and-Excitation (SE)}

SE blocks inserted after each residual block with reduction ratio r=16. Channel-wise recalibration via global pooling→FC(C/16)→ReLU→FC(C)→Sigmoid. Minimal computational overhead (<2\% parameters).

\subsubsection{CBAM}

CBAM applies sequential channel and spatial attention. Channel: Max+Avg pooling→shared MLP. Spatial: Max+Avg pooling→7×7 conv→sigmoid. Inserted after layers 1-4 in ResNet-50.

\section{Training Configuration}

\subsection{Hyperparameters}

Consistent across all experiments:
\begin{itemize}
    \item Optimizer: AdamW
    \item Learning rate: 5×10$^{-4}$
    \item Weight decay: 1×10$^{-2}$
    \item Batch size: 64
    \item Epochs: 30
    \item Loss function: Cross-entropy
\end{itemize}

\subsection{Reproducibility}

Random seeds: 42, 123, 999. All experiments run with 3 seeds. Results report mean ± standard deviation.

\section{Evaluation Metrics}

\subsection{Classification Accuracy}

Standard top-1 accuracy: Accuracy = Correct Predictions / Total Predictions

\subsection{Macro-averaged F1 Score}

Treats all classes equally, addressing class imbalance.

\subsection{Top-3 Accuracy}

Measures whether correct class appears in top 3 predictions.

\subsection{Confusion Matrix}

Per-class confusion matrices identify systematic misclassification patterns.

%%%%%%%%%%%%%%%%%%%%%%
%%% Implementation
%%%%%%%%%%%%%%%%%%%%%%
%%% Implementation
\chapter{Implementation}

This chapter details the software implementation of the project, including the system architecture, code structure, and key technical challenges encountered during development.

\section{System Architecture}

The project is implemented in Python 3.12 using the PyTorch deep learning framework. The codebase follows a modular design to ensure extensibility and reproducibility.

\subsection{Directory Structure}

The project is organized into logical components:
\begin{itemize}
    \item \texttt{product/models/}: Contains model definitions (CBAM, SE blocks).
    \item \texttt{product/training/}: Training scripts for each architecture (Baseline, AlexNet, ResNet).
    \item \texttt{product/artifacts/}: Stores datasets, splits, and experiment results.
    \item \texttt{notebooks/}: Jupyter notebooks for data exploration and visualization.
\end{itemize}

\section{Data Pipeline}

Efficient data loading is critical for training deep learning models. We implemented a custom \texttt{SpectrogramCSVDataset} class inheriting from PyTorch's \texttt{Dataset}.

\subsection{On-the-Fly Loading}
Instead of loading all spectrograms into memory, the dataset reads images on-demand from disk. This allows for scalability to larger datasets. The pipeline includes:
\begin{enumerate}
    \item Reading file paths and labels from a CSV manifest.
    \item Loading the image using PIL (Python Imaging Library).
    \item Converting to Tensor and normalizing with ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).
\end{enumerate}

\section{Training Infrastructure}

\subsection{Experiment Automation}
To ensure robust results, we developed automated batch scripts (\texttt{run\_all\_experiments.bat}) that:
\begin{itemize}
    \item Execute training runs sequentially for multiple models.
    \item Automatically cycle through random seeds (42, 123, 999).
    \item Log all outputs to unique directories timestamped for traceability.
\end{itemize}

\subsection{Logging and Monitoring}
We integrated TensorBoard for real-time monitoring of:
\begin{itemize}
    \item Training and validation loss curves.
    \item Accuracy progression.
    \item Confusion matrices generated at the end of training.
\end{itemize}

\section{Key Technical Challenges}

\subsection{Data Leakage Prevention}
A critical issue was identified early in development where data augmentation caused leakage between training and validation sets. Naively splitting the dataset after augmentation resulted in original and augmented versions of the same clip appearing in both sets, artificially inflating accuracy.

\textbf{Solution:} We implemented a grouped splitting strategy where the split is performed on the unique source file IDs before augmentation. All augmented variants of a source file are then assigned to the same split as the original, ensuring strict separation.

\subsection{Attention Mechanism Integration}
Integrating CBAM into the pre-trained ResNet-50 architecture required careful handling of layer freezing. Initial experiments showed high variance because the added attention modules in early layers were inadvertently frozen while only the final layers were trained.

\textbf{Solution:} We modified the fine-tuning strategy to explicitly unfreeze all added attention modules (CBAM/SE) across all layers, while keeping the pre-trained convolutional weights frozen during the initial phase. This ensured the attention mechanisms could learn effective feature recalibration.

\subsection{Cross-Platform Compatibility}
Developing on Windows required handling path separators and encoding issues (e.g., Unicode characters in logs) that differ from Linux environments. We standardized on using Python's \texttt{pathlib} for robust path handling and ensured all file I/O used explicit UTF-8 encoding.

%%%%%%%%%%%%%%%%%%%%%%
%%% Results
\chapter{Results and Analysis}

\section{Experimental Results}

We present the quantitative results of our experiments on the ESC-50 and EmoDB datasets. Table~\ref{tab:esc50_results} and Table~\ref{tab:emodb_results} summarize the performance of the evaluated architectures.

\subsection{ESC-50 Results}

The experiments on the ESC-50 dataset demonstrate the clear advantage of transfer learning. The Baseline CNN, trained from scratch, achieved a validation accuracy of only 44.44\%. In contrast, the ImageNet pre-trained models performed significantly better.

AlexNet achieved an average accuracy of 74.5\%, while the deeper ResNet-50 model reached 89.87\%. The integration of Squeeze-and-Excitation (SE) blocks into ResNet-50 yielded an average accuracy of 88.77\%, which is statistically comparable to the vanilla ResNet-50. The CBAM-augmented model achieved 85.13\% in our preliminary run.

\begin{table}[h]
\centering
\caption{Performance comparison on ESC-50 dataset (Validation Set).}
\label{tab:esc50_results}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Runs} & \textbf{Avg. Accuracy (\%)} & \textbf{Avg. Macro F1} & \textbf{Best Acc (\%)} \\ \hline
Baseline CNN & 1 & 44.44 & 0.424 & 44.44 \\
AlexNet & 2 & 74.50 & 0.738 & 77.56 \\
ResNet-50 & 3 & 89.87 & 0.898 & 90.88 \\
ResNet-50 + SE & 3 & 88.77 & 0.887 & 91.56 \\
ResNet-50 + CBAM & 1 & 85.13 & 0.851 & 85.13 \\ \hline
\end{tabular}
\end{table}

\subsection{EmoDB Results}

On the EmoDB dataset, both ResNet-50 and ResNet-50 + SE achieved very high performance, indicating that the models can effectively distinguish between emotional states from speech spectrograms.

\begin{table}[h]
\centering
\caption{Performance comparison on EmoDB dataset (Validation Set).}
\label{tab:emodb_results}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Runs} & \textbf{Avg. Accuracy (\%)} & \textbf{Avg. Macro F1} & \textbf{Best Acc (\%)} \\ \hline
ResNet-50 & 2 & 95.72 & 0.946 & 97.98 \\
ResNet-50 + SE & 3 & 95.79 & 0.949 & 96.42 \\ \hline
\end{tabular}
\end{table}

\section{Analysis}

The results highlight several key findings:
\begin{itemize}
    \item \textbf{Impact of Depth and Pre-training:} ResNet-50 outperforms AlexNet by a large margin ($\sim$15\%), confirming that deeper architectures with residual connections are better suited for capturing the complex time-frequency patterns in spectrograms.
    \item \textbf{Effectiveness of Attention:} While SE blocks added a slight improvement in the best-case scenario for ESC-50 (91.56\% vs 90.88\%), the average performance was similar to the baseline ResNet-50. This suggests that for these specific datasets, the global channel re-weighting provided by SE blocks might be redundant given the strong feature extraction of ResNet-50.
    \item \textbf{Generalization:} The high accuracy on EmoDB ($>$95\%) confirms that the transfer learning approach generalizes well to speech emotion recognition tasks.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%
%%% Planning
\chapter{Planning and Timeline}

[To be completed]

%%%%%%%%%%%%%%%%%%%%%%
%%% Diary
\chapter{Project Diary}

[To be completed]


%%%% BIBLIOGRAPHY
\newpage
\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography}

\bibitem{stevens1937scale} Stanley Smith Stevens, John Volkmann, and Edwin B. Newman. \emph{A Scale for the Measurement of the Psychological Magnitude Pitch}. The Journal of the Acoustical Society of America, 1937.

\bibitem{lecun1998gradient} Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. \emph{Gradient-Based Learning Applied to Document Recognition}. Proceedings of the IEEE, 1998.

\bibitem{ioffe2015batch} Sergey Ioffe and Christian Szegedy. \emph{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}. In Proceedings of ICML, 2015.

\bibitem{krizhevsky2012imagenet} Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. \emph{ImageNet Classification with Deep Convolutional Neural Networks}. In Advances in Neural Information Processing Systems (NIPS), 2012.

\bibitem{he2016deep} Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \emph{Deep Residual Learning for Image Recognition}. In Proceedings of CVPR, 2016.

\bibitem{deng2009imagenet} Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \emph{ImageNet: A Large-Scale Hierarchical Image Database}. In Proceedings of CVPR, 2009.

\bibitem{hershey2017cnn} Shawn Hershey et al. \emph{CNN Architectures for Large-Scale Audio Classification}. In Proceedings of ICASSP, 2017.

\bibitem{hu2018squeeze} Jie Hu, Li Shen, and Gang Sun. \emph{Squeeze-and-Excitation Networks}. In Proceedings of CVPR, 2018.

\bibitem{woo2018cbam} Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. \emph{CBAM: Convolutional Block Attention Module}. In Proceedings of ECCV, 2018.

\end{thebibliography}
\label{endpage}

\end{document}
